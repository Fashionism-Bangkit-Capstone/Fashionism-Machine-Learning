{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FG3q3YBlvhzT"
      },
      "outputs": [],
      "source": [
        "\n",
        "# =========\n",
        "# IMPORTING\n",
        "# =========\n",
        "\n",
        "# Importing necessary libraries and modules for the code.\n",
        "\n",
        "import gdown  # Library for downloading datasets\n",
        "import os  # Library for managing file paths\n",
        "import json  # Library for working with JSON files\n",
        "import pickle  # Library for serialization and deserialization of Python objects\n",
        "import re  # Library for regular expressions\n",
        "import cv2  # Library for image processing\n",
        "import concurrent.futures  # Library for parallel computing\n",
        "import random  # Library for random number generation\n",
        "import shutil  # Library for file and directory operations\n",
        "import pandas as pd  # Library for data manipulation and analysis\n",
        "import numpy as np  # Library for numerical operations\n",
        "import matplotlib.pyplot as plt  # Library for plotting images and graphs\n",
        "from PIL import Image  # Library for image processing\n",
        "from google.colab import files  # Library for file upload and download in Google Colab\n",
        "from numpy.linalg import norm  # Library for vector normalization\n",
        "from tqdm import tqdm  # Library for creating progress bars\n",
        "import tensorflow as tf  # Library for deep learning models and operations\n",
        "from keras.preprocessing.image import ImageDataGenerator  # Keras module for image data preprocessing\n",
        "from sklearn.model_selection import train_test_split  # Library for splitting data into train and test sets\n",
        "from sklearn.utils import shuffle  # Library for shuffling data\n",
        "from keras.callbacks import EarlyStopping  # Keras callback for early stopping during model training\n",
        "from keras.models import load_model  # Keras function for loading pre-trained models\n",
        "from keras.layers import GlobalMaxPooling2D  # Keras layer for global max pooling\n",
        "from keras.applications import ResNet50  # Pre-trained CNN model in Keras\n",
        "from keras.applications.resnet import preprocess_input  # Function for preprocessing input images\n",
        "from sklearn.neighbors import NearestNeighbors  # Library for performing nearest neighbor search\n",
        "import matplotlib.image as mpimg  # Library for loading and displaying images"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==================================\n",
        "# DOWNLOAD AND PREPARING THE DATASET\n",
        "# ==================================\n",
        "\n",
        "# Data Download\n",
        "\n",
        "file_id = '1F0oXhVQfmv3qsISc5stW8EiVEV7N-OF_'\n",
        "url = f'https://drive.google.com/uc?id={file_id}'\n",
        "\n",
        "output = '/content/dataset.zip'  # dataset\n",
        "\n",
        "gdown.download(url, output, quiet=False)\n",
        "\n",
        "# Unzip\n",
        "! unzip \"/content/dataset.zip\"\n",
        "\n",
        "# Merge all the json to csv\n",
        "def process_json_file(file_path):\n",
        "    try:\n",
        "        with open(file_path, 'r') as json_file:\n",
        "            json_data = json.load(json_file)\n",
        "\n",
        "            setId = json_data['setId']\n",
        "            user = json_data['user']\n",
        "\n",
        "            processed_items = []\n",
        "            for item in json_data['items']:\n",
        "                itemId = item['itemId']\n",
        "                imgUrl = item['imgUrl']\n",
        "                price = item['price']\n",
        "                itemName = item['itemName']\n",
        "                colors = item['colors']\n",
        "                expressions = item['expressions']\n",
        "                category_x_color = item['category x color']\n",
        "\n",
        "                file_path = os.path.join(base_directory, str(user), str(setId), str(itemId) + '_m.jpg')\n",
        "\n",
        "                processed_items.append({\n",
        "                    'setId': setId,\n",
        "                    'file_path': file_path,\n",
        "                    'price': price,\n",
        "                    'itemName': itemName,\n",
        "                    'expressions': expressions,\n",
        "                    'category_x_color': category_x_color})\n",
        "\n",
        "            return processed_items\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing JSON file: {file_path}\")\n",
        "        print(f\"Error message: {str(e)}\")\n",
        "        return []\n",
        "\n",
        "def convert_json_to_csv(base_directory):\n",
        "    data = []\n",
        "\n",
        "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
        "        # Collect file paths\n",
        "        file_paths = []\n",
        "        for root, dirs, files in os.walk(base_directory):\n",
        "            for file in files:\n",
        "                if file.endswith('.json'):  # Process only JSON files\n",
        "                    file_path = os.path.join(root, file)\n",
        "                    file_paths.append(file_path)\n",
        "\n",
        "        # Process files concurrently\n",
        "        results = executor.map(process_json_file, file_paths)\n",
        "\n",
        "        # Collect processed items from results\n",
        "        for processed_items in results:\n",
        "            data.extend(processed_items)\n",
        "\n",
        "    df = pd.DataFrame(data)\n",
        "    return df\n",
        "\n",
        "base_directory = '/content/IQON3000'\n",
        "df = convert_json_to_csv(base_directory)\n"
      ],
      "metadata": {
        "id": "Nn6QF0yav3Hc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===================\n",
        "# PREPARE THE DATASET\n",
        "# ===================\n",
        "\n",
        "# Create new columns \"category\" and \"color\" in the DataFrame\n",
        "df[['category', 'color']] = df['category_x_color'].str.split(' × ', expand=True)\n",
        "\n",
        "# Map the category to clothes, bottom, and accessories\n",
        "top_categories = ['ニット' , 'ブラウス' , 'コート' , 'ジャケット' , 'カーディガン' , 'パーカー' , 'ダウンジャケット' , 'チュニック' , 'ワンピース']\n",
        "bottom_categories = ['ロングパンツ' , 'スカート' , 'Tシャツ' , 'ショートパンツ' , 'レッグウェア' , 'ロングスカート']\n",
        "accessories_categories = ['ピアス' , 'ショルダーバッグ' , '浴衣' , '水着' , 'タンクトップ' , 'パンプス' , 'トップス' , 'ネックレス' , 'サンダル' , 'アンダーウェア' , 'ブレスレット' , 'スニーカー' , 'インテリア' , 'コスメ' , 'ルームウェア' , '腕時計' , 'トートバッグ' , 'ブーツ' , 'クラッチバッグ' , 'ストール' , 'ハット' , 'ハンドバッグ' , 'サングラス' , 'リング' , 'メガネ' , 'ヘアアクセサリー' , 'リュック' , 'キャップ' , 'バッグ' , 'ベルト' , '靴' , '帽子' , 'フレグランス' , 'ニット帽' , 'ネイル' , 'ボストンバッグ' , '小物' , '財布' , '手袋' , 'ボディケア' , 'ブローチ' , '傘' , 'ファッション小物' , 'ステーショナリー' , 'アクセサリー' , 'ルームシューズ']\n",
        "df['category_mapped'] = df['category'].map(lambda x: 'Top' if x in top_categories else ('Bottom' if x in bottom_categories else 'Accessories'))\n",
        "\n",
        "# Drop the unnecesarry column\n",
        "df.drop(columns=[\"category_x_color\", \"category\", \"color\"], inplace=True)\n",
        "\n",
        "# Drop the accessories\n",
        "df = df[df[\"category_mapped\"] != \"Accessories\"]\n",
        "\n",
        "# Hapus yang punya setId tunggal\n",
        "df = df[~df[\"setId\"].isin(df[\"setId\"].value_counts()[df[\"setId\"].value_counts() == 1].index)]\n",
        "\n",
        "# Save current progress\n",
        "df.to_csv(\"dataset2.csv\", index=False)\n",
        "\n",
        "# Ambil sampel untuk melakukan training\n",
        "\n",
        "# Hapus x% isi dataframe\n",
        "df = df.sample(frac=0.1, random_state=42)\n",
        "\n",
        "# Hapus yang punya setId tunggal\n",
        "df = df[~df[\"setId\"].isin(df[\"setId\"].value_counts()[df[\"setId\"].value_counts() == 1].index)]\n",
        "\n",
        "# Save progress\n",
        "df.to_csv(\"dataset3.csv\", index=False)\n",
        "\n",
        "# dataframe shape\n",
        "df.shape\n",
        "\n",
        "# Check whether the image is corrupted or not\n",
        "def is_image_valid(file_path):\n",
        "    try:\n",
        "        Image.open(file_path)\n",
        "        return True\n",
        "    except:\n",
        "        return False\n",
        "\n",
        "# Check every row\n",
        "valid_rows = []\n",
        "for index, row in df.iterrows():\n",
        "    file_path = row['file_path']\n",
        "    if os.path.exists(file_path) and is_image_valid(file_path):\n",
        "        valid_rows.append(row)\n",
        "\n",
        "# Create valid dataframe\n",
        "df = pd.DataFrame(valid_rows)\n",
        "\n",
        "# Save progress\n",
        "df.to_csv(\"dataset4.csv\", index=False)\n",
        "\n",
        "# shape\n",
        "df.shape"
      ],
      "metadata": {
        "id": "kDoFu8sDwBDJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===============================================\n",
        "# INITIAL TOP/DOWN MODELING TO TEST THE ALGORITHM\n",
        "# ===============================================\n",
        "\n",
        "# Split dataframe menjadi data latih dan data uji\n",
        "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
        "\n",
        "# Definisikan parameter untuk pra-pemrosesan gambar\n",
        "image_size = (150, 150)\n",
        "batch_size = 32\n",
        "\n",
        "# Pra-pemrosesan gambar dan augmentasi data latih\n",
        "train_datagen = ImageDataGenerator(rescale=1./255, validation_split=0.2)  # Normalisasi piksel\n",
        "train_generator = train_datagen.flow_from_dataframe(\n",
        "    dataframe=train_df,\n",
        "    x_col='file_path',\n",
        "    y_col='category_mapped',\n",
        "    target_size=image_size,\n",
        "    batch_size=batch_size,\n",
        "    class_mode='binary',\n",
        "    subset='training'\n",
        ")\n",
        "\n",
        "# Pra-pemrosesan gambar data validasi\n",
        "validation_generator = train_datagen.flow_from_dataframe(\n",
        "    dataframe=train_df,\n",
        "    x_col='file_path',\n",
        "    y_col='category_mapped',\n",
        "    target_size=image_size,\n",
        "    batch_size=batch_size,\n",
        "    class_mode='binary',\n",
        "    subset='validation'\n",
        ")\n",
        "\n",
        "# Membangun model CNN\n",
        "model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(150, 150, 3)),\n",
        "    tf.keras.layers.MaxPooling2D(2, 2),\n",
        "    tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(2, 2),\n",
        "    tf.keras.layers.Conv2D(128, (3, 3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(2, 2),\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(128, activation='relu'),\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# Mengompilasi model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Callback EarlyStopping\n",
        "early_stopping = EarlyStopping(monitor='val_accuracy', patience=3, mode='max', min_delta=0.01)\n",
        "\n",
        "# Melatih model dengan data latih dan evaluasi pada data validasi\n",
        "num_epochs = 100\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    epochs=num_epochs,\n",
        "    validation_data=validation_generator,\n",
        "    callbacks=[early_stopping]\n",
        ")\n",
        "\n",
        "# Evaluasi model pada data uji\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "test_generator = test_datagen.flow_from_dataframe(\n",
        "    dataframe=test_df,\n",
        "    x_col='file_path',\n",
        "    y_col='category_mapped',\n",
        "    target_size=image_size,\n",
        "    batch_size=batch_size,\n",
        "    class_mode='binary'\n",
        ")\n",
        "\n",
        "test_loss, test_accuracy = model.evaluate(test_generator)\n",
        "print('Akurasi pada data uji:', test_accuracy)\n",
        "\n",
        "# Visualisasi loss dan akurasi\n",
        "plt.figure(figsize=(12, 4))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['loss'], label='Training Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Simpan model ke file\n",
        "model.save('model_top_down.h5')\n",
        "\n",
        "# Download the model\n",
        "files.download(\"model_top_down.h5\")"
      ],
      "metadata": {
        "id": "wiistEtZwKp5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =============\n",
        "# MODEL TESTING\n",
        "# =============\n",
        "\n",
        "# Test the model\n",
        "\n",
        "model = load_model('model_top_down.h5')\n",
        "\n",
        "# Upload image\n",
        "uploaded_image = files.upload()\n",
        "\n",
        "# Load and preprocess the uploaded image\n",
        "image_path = list(uploaded_image.keys())[0]\n",
        "image = Image.open(image_path)\n",
        "image = image.resize((150, 150))\n",
        "image = np.array(image) / 255.0\n",
        "image = np.expand_dims(image, axis=0)\n",
        "\n",
        "# Predict the category\n",
        "prediction = model.predict(image)\n",
        "predicted_class = 'top' if prediction[0][0] > 0.5 else 'bottom'\n",
        "\n",
        "# Display the uploaded image\n",
        "plt.imshow(image[0])\n",
        "plt.title(f\"Predicted Category: {predicted_class}\")\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "yll2bqRkwnfC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===============================\n",
        "# RECOMMENDATION DATA PREPARATION\n",
        "# ===============================\n",
        "\n",
        "# Hapus baris dataframe yang tidak memiliki category_mapped top dan bottom\n",
        "\n",
        "df_filtered = df.groupby('setId').filter(lambda x: x['category_mapped'].nunique() == 2)\n",
        "\n",
        "df = df_filtered\n",
        "del df_filtered"
      ],
      "metadata": {
        "id": "rXJ-gw7jwwBk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===========================\n",
        "# FEATURE EXTRACTION AND SAVE\n",
        "# ===========================\n",
        "\n",
        "# Buat list directory top dan bottom\n",
        "\n",
        "top_directory_df = df[df['category_mapped'] == 'Top']\n",
        "bottom_directory_df = df[df['category_mapped'] == 'Bottom']\n",
        "top_directory = df[df['category_mapped'] == 'Top']['file_path'].tolist()\n",
        "bottom_directory = df[df['category_mapped'] == 'Bottom']['file_path'].tolist()\n",
        "\n",
        "# Save to pickle\n",
        "\n",
        "pickle.dump(top_directory, open(\"top_directory.pkl\", \"wb\"))\n",
        "pickle.dump(bottom_directory, open(\"bottom_directory.pkl\", \"wb\"))\n",
        "pickle.dump(top_directory_df, open(\"top_directory_df.pkl\", \"wb\"))\n",
        "pickle.dump(bottom_directory_df, open(\"bottom_directory_df.pkl\", \"wb\"))\n",
        "\n",
        "# Feature extraction\n",
        "resnet_model = ResNet50(weights='imagenet', include_top=False, input_shape=(150, 150, 3))\n",
        "resnet_model.trainable = False\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "    resnet_model,\n",
        "    GlobalMaxPooling2D()\n",
        "])\n",
        "\n",
        "model.save(\"feature_extract_model.h5\")\n",
        "\n",
        "def extract_feature(img_path, model):\n",
        "  img = cv2.imread(img_path)\n",
        "  img = cv2.resize(img, (150,150))\n",
        "  img = np.array(img)\n",
        "  expand_img = np.expand_dims(img, axis=0)\n",
        "  pre_img = preprocess_input(expand_img)\n",
        "  result = model.predict(pre_img).flatten()\n",
        "  normalized = result/norm(result)\n",
        "  return normalized\n",
        "\n",
        "# Feature extraction untuk fitur \"top\"\n",
        "\n",
        "feature_extract_model = load_model(\"feature_extract_model.h5\")\n",
        "top_image_features = [extract_feature(file, feature_extract_model) for file in tqdm(top_directory[\"file_path\"])]\n",
        "\n",
        "# Feature extraction untuk fitur \"bottom\"\n",
        "\n",
        "feature_extract_model = load_model(\"feature_extract_model.h5\")\n",
        "bottom_image_features = [extract_feature(file, feature_extract_model) for file in tqdm(bottom_directory[\"file_path\"])]\n",
        "\n",
        "# Export the feature to pickle file\n",
        "\n",
        "pickle.dump(top_image_features, open(\"top_feature_extraction.pkl\", \"wb\"))\n",
        "pickle.dump(bottom_image_features, open(\"bottom_feature_extraction.pkl\", \"wb\"))\n",
        "\n",
        "\n",
        "files.download(\"top_feature_extraction.pkl\")\n",
        "files.download(\"bottom_feature_extraction.pkl\")"
      ],
      "metadata": {
        "id": "w3Fjy_Scw4Oh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===================================\n",
        "# RECOMMENDATION MODELING AND TESTING\n",
        "# ===================================\n",
        "\n",
        "# Recommend model\n",
        "def recommend(features,feature_list):\n",
        "    neighbors = NearestNeighbors(n_neighbors=6, algorithm='brute', metric='euclidean')\n",
        "    neighbors.fit(feature_list)\n",
        "\n",
        "    distances, indices = neighbors.kneighbors([features])\n",
        "\n",
        "    return indices\n",
        "\n",
        "# Load all the model and extracted features\n",
        "\n",
        "model_top_down = load_model(\"model_top_down.h5\")\n",
        "model_extraction = load_model(\"feature_extract_model.h5\")\n",
        "\n",
        "top_feature_list = np.array(pickle.load(open(\"top_feature_extraction.pkl\", \"rb\")))\n",
        "bottom_feature_list = np.array(pickle.load(open(\"bottom_feature_extraction.pkl\", \"rb\")))\n",
        "\n",
        "top_filenames = pickle.load(open('top_directory.pkl', \"rb\"))\n",
        "bottom_filenames = pickle.load(open(\"bottom_directory.pkl\", \"rb\"))\n",
        "\n",
        "top_filenames_df = pickle.load(open('top_directory_df.pkl', 'rb'))\n",
        "bottom_filenames_df = pickle.load(open('bottom_directory_df.pkl', 'rb'))\n",
        "\n",
        "# Upload image\n",
        "\n",
        "# Create the folder if it doesn't exist\n",
        "folder_path = '/content/upload'\n",
        "os.makedirs(folder_path, exist_ok=True)\n",
        "\n",
        "# Upload image\n",
        "uploaded_image = files.upload()\n",
        "\n",
        "# Save the uploaded image to the folder\n",
        "image_path = list(uploaded_image.keys())[0]\n",
        "new_image_path = os.path.join(folder_path, image_path)\n",
        "shutil.move(image_path, new_image_path)\n",
        "\n",
        "# Load and preprocess the uploaded image\n",
        "image_path = new_image_path\n",
        "image = Image.open(image_path)\n",
        "image = image.resize((150, 150))\n",
        "image = np.array(image) / 255.0\n",
        "image = np.expand_dims(image, axis=0)\n",
        "\n",
        "# Predict the category\n",
        "prediction = model_top_down.predict(image)\n",
        "predicted_class = 'top' if prediction[0][0] > 0.5 else 'bottom'\n",
        "\n",
        "# Display the uploaded image\n",
        "plt.imshow(image[0])\n",
        "plt.title(f\"Predicted Category: {predicted_class}\")\n",
        "plt.axis('off')\n",
        "plt.show()\n",
        "\n",
        "# Do the recommendation if predicted is top\n",
        "\n",
        "if predicted_class == 'top':\n",
        "  features = extract_feature(image_path, model_extraction)\n",
        "  indices = recommend(features, top_feature_list)\n",
        "\n",
        "  # Recommendation file_path\n",
        "  target_file = top_filenames[indices[0][0]]\n",
        "\n",
        "  # Ambil setId dari rekomendasi\n",
        "  filtered_df = df.loc[df['file_path'] == target_file]\n",
        "\n",
        "  set_id = filtered_df[\"setId\"]\n",
        "\n",
        "  for set in set_id:\n",
        "    set_id = set\n",
        "    break\n",
        "\n",
        "  # Teruskan setId ke dataframe bottom\n",
        "  recommended_path = bottom_filenames_df[bottom_filenames_df['setId'] == set_id][\"file_path\"]\n",
        "\n",
        "  for path in recommended_path:\n",
        "    recommended_path = path\n",
        "    break\n",
        "\n",
        "  # Recommend the bottom\n",
        "  features = extract_feature(recommended_path, model_extraction)\n",
        "  indices = recommend(features, bottom_feature_list)\n",
        "\n",
        "  # Print the recommendation\n",
        "  target_files = [\n",
        "      bottom_filenames[indices[0][0]],\n",
        "      bottom_filenames[indices[0][1]],\n",
        "      bottom_filenames[indices[0][2]],\n",
        "      bottom_filenames[indices[0][3]],\n",
        "      bottom_filenames[indices[0][4]]\n",
        "  ]\n",
        "\n",
        "  price_output = []\n",
        "\n",
        "  for file in target_files:\n",
        "      filtered_df = bottom_filenames_df.loc[bottom_filenames_df['file_path'] == file]\n",
        "      prices = filtered_df[\"price\"].values\n",
        "      if len(prices) > 0:\n",
        "          price_output.append(prices[0])\n",
        "      else:\n",
        "          price_output.append('Unknown')\n",
        "\n",
        "  # Iterate over the target files and display the images with labels\n",
        "  for i, target_file in enumerate(target_files):\n",
        "      image_path = target_file\n",
        "      img = mpimg.imread(image_path)\n",
        "      plt.imshow(img)\n",
        "      plt.axis('off')\n",
        "      plt.title(f\"Price: {price_output[i]}\")\n",
        "      plt.show()\n",
        "\n",
        "\n",
        "# If the input is bottom\n",
        "\n",
        "else:\n",
        "  features = extract_feature(image_path, model_extraction)\n",
        "  indices = recommend(features, bottom_feature_list)\n",
        "\n",
        "  # Recommendation file_path\n",
        "  target_file = bottom_filenames[indices[0][0]]\n",
        "\n",
        "  # Ambil setId dari rekomendasi\n",
        "  filtered_df = df.loc[df['file_path'] == target_file]\n",
        "\n",
        "  set_id = filtered_df[\"setId\"]\n",
        "\n",
        "  for set in set_id:\n",
        "    set_id = set\n",
        "    break\n",
        "\n",
        "  # Teruskan setId ke dataframe top\n",
        "  recommended_path = top_filenames_df[top_filenames_df['setId'] == set_id][\"file_path\"]\n",
        "\n",
        "  for path in recommended_path:\n",
        "    recommended_path = path\n",
        "    break\n",
        "\n",
        "  # Recommend the top\n",
        "  features = extract_feature(recommended_path, model_extraction)\n",
        "  indices = recommend(features, top_feature_list)\n",
        "\n",
        "  # Print the recommendation\n",
        "  target_files = [\n",
        "      top_filenames[indices[0][0]],\n",
        "      top_filenames[indices[0][1]],\n",
        "      top_filenames[indices[0][2]],\n",
        "      top_filenames[indices[0][3]],\n",
        "      top_filenames[indices[0][4]]\n",
        "  ]\n",
        "\n",
        "  price_output = []\n",
        "\n",
        "  for file in target_files:\n",
        "      filtered_df = top_filenames_df.loc[top_filenames_df['file_path'] == file]\n",
        "      prices = filtered_df[\"price\"].values\n",
        "      if len(prices) > 0:\n",
        "          price_output.append(prices[0])\n",
        "      else:\n",
        "          price_output.append('Unknown')\n",
        "\n",
        "  # Iterate over the target files and display the images with labels\n",
        "  for i, target_file in enumerate(target_files):\n",
        "      image_path = target_file\n",
        "      img = mpimg.imread(image_path)\n",
        "      plt.imshow(img)\n",
        "      plt.axis('off')\n",
        "      plt.title(f\"Price: {price_output[i]}\")\n",
        "      plt.show()"
      ],
      "metadata": {
        "id": "CcDTjtZixA6t"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}