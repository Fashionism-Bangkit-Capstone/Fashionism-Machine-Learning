{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import\n",
    "import os\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from keras.applications.resnet import preprocess_input\n",
    "from numpy.linalg import norm # Normalize\n",
    "from keras.models import load_model\n",
    "import pickle\n",
    "from flask import Flask, request, jsonify\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================\n",
    "#     FUNCTIONS\n",
    "# =====================\n",
    "\n",
    "def recommend(features,feature_list):\n",
    "    neighbors = NearestNeighbors(n_neighbors=6, algorithm='brute', metric='euclidean')\n",
    "    neighbors.fit(feature_list)\n",
    "\n",
    "    distances, indices = neighbors.kneighbors([features])\n",
    "\n",
    "    return indices\n",
    "\n",
    "def extract_feature(img_path, model):\n",
    "  img = cv2.imread(img_path)\n",
    "  img = cv2.resize(img, (150,150))\n",
    "  img = np.array(img)\n",
    "  expand_img = np.expand_dims(img, axis=0)\n",
    "  pre_img = preprocess_input(expand_img)\n",
    "  result = model.predict(pre_img).flatten()\n",
    "  normalized = result/norm(result)\n",
    "  return normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    }
   ],
   "source": [
    "# =================\n",
    "# MODELS IMPORT\n",
    "# =================\n",
    "\n",
    "model_top_down = load_model(\"Models/model_top_down.h5\")\n",
    "model_extraction = load_model(\"Models/feature_extract_model.h5\")\n",
    "\n",
    "top_feature_list = np.array(pickle.load(open(\"Models/top_feature_extraction.pkl\", \"rb\")))\n",
    "bottom_feature_list = np.array(pickle.load(open(\"Models/bottom_feature_extraction.pkl\", \"rb\")))\n",
    "\n",
    "top_filenames = pickle.load(open('Models/top_directory.pkl', \"rb\"))\n",
    "bottom_filenames = pickle.load(open(\"Models/bottom_directory.pkl\", \"rb\"))\n",
    "\n",
    "top_filenames_df = pickle.load(open('Models/top_directory_df.pkl', 'rb'))\n",
    "bottom_filenames_df = pickle.load(open('Models/bottom_directory_df.pkl', 'rb'))\n",
    "\n",
    "top_url = pickle.load(open(\"Models/top_url.pkl\", \"rb\"))\n",
    "bottom_url = pickle.load(open(\"Models/bottom_url.pkl\", \"rb\"))\n",
    "\n",
    "df = pd.read_csv(\"dataset8.csv\", index_col=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 38ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top\n"
     ]
    }
   ],
   "source": [
    "image_path = \"Dataset//7476018_m.jpg\"\n",
    "\n",
    "# Image preprocessing\n",
    "image = Image.open(image_path)\n",
    "image = image.resize((150, 150))\n",
    "image = np.array(image) / 255.0\n",
    "image = np.expand_dims(image, axis=0)\n",
    "\n",
    "# Predict the category\n",
    "prediction = model_top_down.predict(image)\n",
    "predicted_class = 'top' if prediction[0][0] > 0.5 else 'bottom'\n",
    "\n",
    "print(predicted_class)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 207ms/step\n",
      "3996102\n",
      "Dataset/39659948_m.jpg\n",
      "1/1 [==============================] - 0s 132ms/step\n",
      "['https://storage.googleapis.com/fashionism_dataset/39659948_m.jpg', 'https://storage.googleapis.com/fashionism_dataset/40314740_m.jpg', 'https://storage.googleapis.com/fashionism_dataset/44584501_m.jpg', 'https://storage.googleapis.com/fashionism_dataset/34139023_m.jpg', 'https://storage.googleapis.com/fashionism_dataset/14236887_m.jpg']\n",
      "[310911.76470588235, 130435.29411764706, 87979.41176470589, 64711.76470588235, 140517.64705882352]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "if predicted_class == 'top':\n",
    "    features = extract_feature(image_path, model_extraction)\n",
    "    indices = recommend(features, top_feature_list)\n",
    "\n",
    "    # Recommendation file_path\n",
    "    target_file = random.choice([top_filenames[indices[0][0]], top_filenames[indices[0][1]], top_filenames[indices[0][2]], top_filenames[indices[0][3]], top_filenames[indices[0][4]]])\n",
    "\n",
    "    # Ambil setId dari rekomendasi\n",
    "    filtered_df = df.loc[df['file_path'] == target_file]\n",
    "\n",
    "    set_id = filtered_df[\"setId\"]\n",
    "\n",
    "    for set in set_id:\n",
    "        set_id = set\n",
    "        break\n",
    "\n",
    "    print(set_id)\n",
    "\n",
    "    # Teruskan setId ke dataframe bottom\n",
    "    recommended_path = bottom_filenames_df[bottom_filenames_df['setId'] == set_id][\"file_path\"]\n",
    "\n",
    "    for path in recommended_path:\n",
    "        recommended_path = path\n",
    "        break\n",
    "\n",
    "    print(recommended_path)\n",
    "\n",
    "    # Recommend the bottom\n",
    "    features = extract_feature(recommended_path, model_extraction)\n",
    "    indices = recommend(features, bottom_feature_list)\n",
    "\n",
    "    # Print the recommendation\n",
    "    target_files = [\n",
    "        bottom_filenames[indices[0][0]],\n",
    "        bottom_filenames[indices[0][1]],\n",
    "        bottom_filenames[indices[0][2]],\n",
    "        bottom_filenames[indices[0][3]],\n",
    "        bottom_filenames[indices[0][4]],\n",
    "    ]\n",
    "\n",
    "    price_output = []\n",
    "    target_link = []\n",
    "\n",
    "    for file in target_files:\n",
    "        filtered_df = bottom_filenames_df.loc[bottom_filenames_df['file_path'] == file]\n",
    "        prices = filtered_df[\"price\"].values\n",
    "        if len(prices) > 0:\n",
    "            price_output.append(prices[0])\n",
    "        else:\n",
    "            price_output.append('Unknown')\n",
    "        for target in filtered_df[\"url\"]:\n",
    "            target=target\n",
    "            break\n",
    "        target_link.append(target)\n",
    "\n",
    "    print(target_link)\n",
    "    print(price_output)\n",
    "\n",
    "\n",
    "else:\n",
    "    features = extract_feature(image_path, model_extraction)\n",
    "    indices = recommend(features, bottom_feature_list)\n",
    "\n",
    "    # Recommendation file_path\n",
    "    target_file = bottom_filenames[indices[0][0]]\n",
    "\n",
    "    # Ambil setId dari rekomendasi\n",
    "    filtered_df = df.loc[df['file_path'] == target_file]\n",
    "\n",
    "    set_id = filtered_df[\"setId\"]\n",
    "\n",
    "    for set in set_id:\n",
    "        set_id = set\n",
    "        break\n",
    "\n",
    "    # Teruskan setId ke dataframe top\n",
    "    recommended_path = top_filenames_df[top_filenames_df['setId'] == set_id][\"file_path\"]\n",
    "\n",
    "    for path in recommended_path:\n",
    "        recommended_path = path\n",
    "        break\n",
    "\n",
    "    # Recommend the top\n",
    "    features = extract_feature(recommended_path, model_extraction)\n",
    "    indices = recommend(features, top_feature_list)\n",
    "\n",
    "    # Print the recommendation\n",
    "    target_files = [\n",
    "        top_filenames[indices[0][0]],\n",
    "        top_filenames[indices[0][1]],\n",
    "        top_filenames[indices[0][2]],\n",
    "        top_filenames[indices[0][3]],\n",
    "        top_filenames[indices[0][4]],\n",
    "    ]\n",
    "\n",
    "    price_output = []\n",
    "    target_link = []\n",
    "\n",
    "    for file in target_files:\n",
    "        filtered_df = top_filenames_df.loc[top_filenames_df['file_path'] == file]\n",
    "        prices = filtered_df[\"price\"].values\n",
    "        if len(prices) > 0:\n",
    "            price_output.append(prices[0])\n",
    "        else:\n",
    "            price_output.append('Unknown')\n",
    "        for target in filtered_df[\"url\"]:\n",
    "            target=target\n",
    "            break\n",
    "        target_link.append(target)\n",
    "\n",
    "    print(target_link)\n",
    "    print(price_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>setId</th>\n",
       "      <th>file_path</th>\n",
       "      <th>price</th>\n",
       "      <th>itemName</th>\n",
       "      <th>expressions</th>\n",
       "      <th>category_mapped</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>660</th>\n",
       "      <td>4011823</td>\n",
       "      <td>Dataset/40314740_m.jpg</td>\n",
       "      <td>130435.294118</td>\n",
       "      <td>スキニージーンズ</td>\n",
       "      <td>['アイテム説明-【送料】商品のサイズや重量、配送ルートよって異なる\\r\\n【配送期間】3～...</td>\n",
       "      <td>Bottom</td>\n",
       "      <td>https://storage.googleapis.com/fashionism_data...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       setId               file_path          price  itemName  \\\n",
       "660  4011823  Dataset/40314740_m.jpg  130435.294118  スキニージーンズ   \n",
       "\n",
       "                                           expressions category_mapped  \\\n",
       "660  ['アイテム説明-【送料】商品のサイズや重量、配送ルートよって異なる\\r\\n【配送期間】3～...          Bottom   \n",
       "\n",
       "                                                   url  \n",
       "660  https://storage.googleapis.com/fashionism_data...  "
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bottom_filenames_df[bottom_filenames_df['url'] == \"https://storage.googleapis.com/fashionism_dataset/40314740_m.jpg\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
